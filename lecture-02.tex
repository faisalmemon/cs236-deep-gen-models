\documentclass{article}
% allow import of figures
\usepackage{import}
% allow inkscape to use colours
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{breqn} 

\title{CS236 Lecture 2}

\begin{document}
\maketitle
\section{The problem space}
\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{probability_distribution.pdf_tex}
    \end{normalsize}
    \label{fig:probability_distribution}
\end{figure}


\begin{itemize}
    \item Over on the left, the blue represents the real world system from which we have data samples.  It could be a set of images of dogs, for example.  So $ x_{i} \sim P_{data} $
    \item The green area is the space of possibilities of probability distributions that are parameterised by $ \theta$.
    \item We need to define the notion of distance or loss function $d$.
\end{itemize}

We want to learn a probability distribution $ p(x) $ over images $x$ such that:
\begin{itemize}
    \item \textbf{Generation:} If we sample $ x_{new} \sim p(x) $, $ x_{new} $ should look like a dog (\emph{sampling})
    \item \textbf{Density estimation:} $ p(x) $ should be high if $x$ looks like a dog, and low otherwise (\emph{anomaly detection})
    \item \textbf{Unsupervised representation learning:} We should be able to learn what these images have in common, e.g. ears, tails, etc. (\emph{features})
\end{itemize}

\section{How do you represent the probability distribution $p(x)$?}

For low dimensional data it would be straightforward.  In the simplest case, binary distribution biased coin flip (Heads or Tails) is modelled by the Bernoulli Distribution.
\begin{itemize}
    \item $ D = \{ Heads, Tails \} $
    \item Specify $ P(X = Heads) = p $. Then $ P(X = Tails) = 1 - p $
    \item Write $ X \sim Ber(p) $
    \item Sampling: flip a (biased) coin

\end{itemize}

Extending this, we can have a Categorical distribution: a biased m-sided dice.
\begin{itemize}
    \item $ D = \{ 1, \cdots, m \} $
    \item $ P(Y=i) = p_{i} $ such that $ \sum p_{i} = 1 $
    \item Write $ Y \sim Cat(p_{1}, \cdots, p_{m}) $
    \item Sampling: roll a biased die
\end{itemize}

\subsection{Images}
When we consider images, they are comprised of pixels.  And each pixel can be considered three random variables, R, G, B each of which is taken from $ \{ 0, \cdots , 255 \} $
Sampling from the joint distribution $ (r, g, b) \sim p(R, G, B) $ randomly generates a colour for the pixel.
The number of parameters to represent this space is $ 256 * 256 * 256 - 1 $.
We have the $ -1 $ because the probabilities we know have to add up to one, so the last parameter is implicitly known by subracting from 1 the sum of all the other parameters.
The problem is that for modest images, the number of parameters is too large.  E.g. $ 23 $ x $ 23 $ pixels black and white for representing a handwritten digit means $ 2^{529} -1 $ parameters.
We need to have some simplifying assumptions.

\section{Structure through independence}

\end{document}