\documentclass{article}
% allow import of figures
\usepackage{import}
% allow inkscape to use colours
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{breqn} 

\title{CS236 Lecture 2}

\begin{document}
\maketitle
\section{The problem space}
\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{probability_distribution.pdf_tex}
    \end{normalsize}
    \label{fig:probability_distribution}
\end{figure}


\begin{itemize}
    \item Over on the left, the blue represents the real world system from which we have data samples.  It could be a set of images of dogs, for example.  So $ x_{i} \sim P_{data} $
    \item The green area is the space of possibilities of probability distributions that are parameterised by $ \theta$.
    \item We need to define the notion of distance or loss function $d$.
\end{itemize}

We want to learn a probability distribution $ p(x) $ over images $x$ such that:
\begin{itemize}
    \item \textbf{Generation:} If we sample $ x_{new} \sim p(x) $, $ x_{new} $ should look like a dog (\emph{sampling})
    \item \textbf{Density estimation:} $ p(x) $ should be high if $x$ looks like a dog, and low otherwise (\emph{anomaly detection})
    \item \textbf{Unsupervised representation learning:} We should be able to learn what these images have in common, e.g. ears, tails, etc. (\emph{features})
\end{itemize}

\section{How do you represent the probability distribution $p(x)$?}

For low dimensional data it would be straightforward.  In the simplest case, binary distribution biased coin flip (Heads or Tails) is modelled by the Bernoulli Distribution.
\begin{itemize}
    \item $ D = \{ Heads, Tails \} $
    \item Specify $ P(X = Heads) = p $. Then $ P(X = Tails) = 1 - p $
    \item Write $ X \sim Ber(p) $
    \item Sampling: flip a (biased) coin

\end{itemize}

Extending this, we can have a Categorical distribution: a biased m-sided dice.
\begin{itemize}
    \item $ D = \{ 1, \cdots, m \} $
    \item $ P(Y=i) = p_{i} $ such that $ \sum p_{i} = 1 $
    \item Write $ Y \sim Cat(p_{1}, \cdots, p_{m}) $
    \item Sampling: roll a biased die
\end{itemize}

\subsection{Images}
When we consider images, they are comprised of pixels.  And each pixel can be considered three random variables, R, G, B each of which is taken from $ \{ 0, \cdots , 255 \} $
Sampling from the joint distribution $ (r, g, b) \sim p(R, G, B) $ randomly generates a colour for the pixel.
The number of parameters to represent this space is $ 256 * 256 * 256 - 1 $.
We have the $ -1 $ because the probabilities we know have to add up to one, so the last parameter is implicitly known by subracting from 1 the sum of all the other parameters.
The problem is that for modest images, the number of parameters is too large.  E.g. $ 23 $ x $ 23 $ pixels black and white for representing a handwritten digit means $ 2^{529} -1 $ parameters.
We need to have some simplifying assumptions.

\section{Structure through independence}

If $ X_{1}, \cdots , X_{n} $ are independent, then
\begin{align}
p(x_{1}, \cdots , x_{n}) &= p(x_{1})p(x_{2})\cdots p(x_{n})
\end{align}

With this assumption how many states are needed for the images? 
It is $ 2^{n} $ where $n$ is the number of pixels.
If the images are black and white, $ |Val(X_{i})| = 2 $. So $2^{n}$ entries can be described by just $n$ numbers.

In reality the independence assumption is too strong.  Because for digits handwritten, you won't get random scattering of pixels.  They will be lit up in a localised area representing the stroke of the pen.

What we do is instead to make conditional independence assumptions.

\begin{enumerate}
    \item \textbf{Chain rule} Let $ S_{1}, \cdots S_{n} $ be events, $ p(S_{i}) > 0 $.
    \newline
    $ p(S_{1} \cap S_{2} \cap \cdots \cap S_{n}) = p(S_{1})p(S_{2} | S_{1}) \cdots p(S_{n} | S_{1} \cap \cdots \cap S_{n-1})  $
    \item \textbf{Bayes' rule} Let $ S_{1}, S_{2} $ be events, $ p(S_{1}) > 0 $ and $ p(S_{2}) > 0 $.
    \newline
    $ p (S_{1} | S_{2}) = \frac{p(S_{1} \cap S_{2})}{p(S_{2})} = \frac{p(S_{2} | S_{1})p(S_{1})}{p(S_{2})}$
\end{enumerate}

In the Chain Rule, the joint distributions on the left are hard to know but the conditional probabilities (aka marginal probabilities) terms on the right are easier to obtain.  This is why the Chain Rule helps us.

Using Chain Rule
\begin{align}
    p(x_{1}, \cdots , x_{n}) &= p(x_{1})p(x_{2} | x_{1})p(x_{3} | x_{1},x_{2}) \cdots p(x_{n} | x_{1}, \cdots , x_{n-1})
\end{align}

This is the kind of factorisation used in autoregressive models.
In fact, any collection of random variables can be factored in this way.

How many parameters do we now need?

$ 1 + 2 + 4 + \cdots + 2^{n-1} = 2^{n} - 1$

The reason why this is a geometric series is because with $ x_{i} $ being $0$ or $1$ then $ x_{1},x_{2} $ means four possibilities so $ p(x_{3} | x_{1}, x_{2}) $ requires 4 parameters. So we have not yet made progress on simplifying our computation.

Now let us suppose that for $ X_{i+1} $ where $X_{i+1}$ represents the weather on day $ i + 1 $ is dependent on only the weather on day $ i $ and independent of the weather on prior days $ 1, \cdots , i - 1 $.  Then we can write this as
$ X_{i+1} \bot \, X_{1}, \cdots X_{i-1} \, | \, X_{i} $

This allows us to simplify because for example the third term 
$ p(x_{3} | x_{1}, x_{2}) $ becomes $ p(x_{3} | x_{2}) $.
This is true for the second to the $ (n - 1) th$ term.  
Now we are conditioning on at most one variable.
Each of those terms requires two parameters; one for the $ x_{i} = 0 $ case and one for the $ x_{i} = 1 $ case.
The first term requires just one parameter since it is one variable alone.

How many parameters do we now need?
$ 1 + \underbrace{2 + 2 + \cdots + 2}_{n -1} = 2n - 1 $

This is a exponential improvement but the predictions may not be good as just using the prior event alone is not enough context for good prediction, such as predicting the next word in a sentence.

\section{Bayes' Network}

Here we use conditional parameterisation instead of joint parameterisation.
For each random variable $ X_{i} $ specify $ p(x_{i}|\boldsymbol{x}_{\boldsymbol{A_{i}}}) $ for set $ \boldsymbol{X_{A_{i}}} $ of random variables.

Then the joint parameterisation is
$ p(x_{1}, \cdots , x_{n}) = \prod_{i} p(x_{i} | \boldsymbol{x}_{\boldsymbol{A_{i}}}))  $

We define a \textbf{Bayesian network} as a \textit{directed} \textbf{acyclic} graph (DAG)
\begin{align}  
    G &= (V, E)
\end{align}

\begin{itemize}
    \item One node $ i \in V $ for each random variable $ X_{i} $
    \item One node for each pixel in images, one node for each word in sentences
    \item One conditional probability distribution (CPD) per node, $ p(x_{i} | \boldsymbol{x}_{Pa(i)}) $, specifying the variable's probability conditioned on its parents' values
\end{itemize}

\begin{align}
    p(x_{1}, \cdots , x_{n}) &= \prod_{i \in V} p(x_{i} | \boldsymbol{x}_{Pa(i)}))
\end{align}

\begin{itemize}
    \item We claim that $ p(x_{1}, \cdots , x_{n}) $ is a valid probability distribution because of ordering implied by DAG.
    \item \textbf{Economical representation:} exponential in $ |Pa(i)| $ not $ |V| $
\end{itemize}

\section{Course Grade Example}
\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{bayes_network.pdf_tex}
    \end{normalsize}
    \label{fig:Bayes Network for course grading}
\end{figure}

Using the terminology d, i, g, s, l for difficulty, intelligence, grade, SAT, and letter respectively, the joint distribution is:

\begin{align}
    p(x_{1}, \cdots , x_{n}) &= \prod_{i \in V} p(x_{i} | \boldsymbol{x}_{Pa(i)}) \\
    p(d,i,g,s,l) &= p(d)p(i)p(g | i,d)p(s | i)p(l | g)
\end{align}

However, it is always true, by the chain rule, \textit{any} distribution can be written as

\begin{align}
    p(d, i, g, s, l) &= p (d)p(i | d)p(s | i, d, g)p(l | g,d,i, s)
\end{align}

Therefore, we are assuming the following additional independencies:

\begin{itemize}
    \item $ D \bot I $
    \item $ S \bot \{D, G\} | I $
    \item $ L \bot \{ I, D, S \} | G $
\end{itemize}

\section{Discriminative model}

What is the difference between the discriminative model approach and the generative approach?  We will look at the discriminative case here for e-mail spam detection.

Classify e-mails as spam $(Y = 1)$ or not spam $(Y = 0)$
\begin{itemize}
    \item Let $ 1 : n $ index the words in our vocabulary (e.g. English)
    \item $ X_{i} = 1 $ if word $i$ appears in an e-mail, and $0$ otherwise
    \item E-mails are drawn according to some distribution $ p(Y, X_{1}, \cdots , X_{n})$
\end{itemize}

The naive Bayes for this setup is to make the words conditionally independent given $Y$:

\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{naive_bayes.pdf_tex}
    \end{normalsize}
    \label{fig:Naive Bayes for Spam detection}
\end{figure}

Then

\begin{align}
    p(y, x_{1}, \cdots , x_{n}) &= p(y) \prod_{i=1}^{n}p(x_{i}\,| \,y)
\end{align}

We \textbf{estimate} the parameters from training data.
We \textbf{predict} with Bayes rule:

\begin{align}
    p(Y = 1 \, | \, x_{1}, \cdots , x_{n}) &= 
     \frac{p(Y=1)\prod_{i=1}^{n}p(x_{i} \, | \, Y = 1)}{\sum_{y=\{0,1\}}p(Y=y)\prod_{i=1}^{n}p(x_{i} \, | \, Y = y)}
\end{align}

Are these independence assumptions reasonable?
Philosophy: Nearly all probablistic models are ``wrong'' but are nonetheless useful.

\section{Descriminative versus generative models}

Here we explain the differences between the generative and the descriminative approaches.

Using the chain rule $p(Y, \boldsymbol{X}) = p(\boldsymbol{X} | Y)p(Y) = p(Y | \boldsymbol{X})p(\boldsymbol{X}) $.
This gives rise to the corresponding Bayesian networks:

\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{desc_vs_gen.pdf_tex}
    \end{normalsize}
    \label{fig:Descriminative versus generative}
\end{figure}

\end{document}