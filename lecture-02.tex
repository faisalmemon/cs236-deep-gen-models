\documentclass{article}
% allow import of figures
\usepackage{import}
% allow inkscape to use colours
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{breqn} 

\title{CS236 Lecture 2}

\begin{document}
\maketitle
\section{The problem space}
\begin{figure}[hbt!]
    \centering
    \begin{normalsize}
        \import{figs/}{probability_distribution.pdf_tex}
    \end{normalsize}
    \label{fig:probability_distribution}
\end{figure}


\begin{itemize}
    \item Over on the left, the blue represents the real world system from which we have data samples.  It could be a set of images of dogs, for example.  So $ x_{i} \sim P_{data} $
    \item The green area is the space of possibilities of probability distributions that are parameterised by $ \theta$.
    \item We need to define the notion of distance or loss function $d$.
\end{itemize}

We want to learn a probability distribution $ p(x) $ over images $x$ such that:
\begin{itemize}
    \item \textbf{Generation:} If we sample $ x_{new} \sim p(x) $, $ x_{new} $ should look like a dog (\emph{sampling})
    \item \textbf{Density estimation:} $ p(x) $ should be high if $x$ looks like a dog, and low otherwise (\emph{anomaly detection})
    \item \textbf{Unsupervised representation learning:} We should be able to learn what these images have in common, e.g. ears, tails, etc. (\emph{features})
\end{itemize}

\section{How do you represent the probability distribution $p(x)$?}

For low dimensional data it would be straightforward.  In the simplest case, binary distribution biased coin flip (Heads or Tails) is modelled by the Bernoulli Distribution.
\begin{itemize}
    \item $ D = \{ Heads, Tails \} $
    \item Specify $ P(X = Heads) = p $. Then $ P(X = Tails) = 1 - p $
    \item Write $ X \sim Ber(p) $
    \item Sampling: flip a (biased) coin

\end{itemize}

Extending this, we can have a Categorical distribution: a biased m-sided dice.
\begin{itemize}
    \item $ D = \{ 1, \cdots, m \} $
    \item $ P(Y=i) = p_{i} $ such that $ \sum p_{i} = 1 $
    \item Write $ Y \sim Cat(p_{1}, \cdots, p_{m}) $
    \item Sampling: roll a biased die
\end{itemize}

\subsection{Images}
When we consider images, they are comprised of pixels.  And each pixel can be considered three random variables, R, G, B each of which is taken from $ \{ 0, \cdots , 255 \} $
Sampling from the joint distribution $ (r, g, b) \sim p(R, G, B) $ randomly generates a colour for the pixel.
The number of parameters to represent this space is $ 256 * 256 * 256 - 1 $.
We have the $ -1 $ because the probabilities we know have to add up to one, so the last parameter is implicitly known by subracting from 1 the sum of all the other parameters.
The problem is that for modest images, the number of parameters is too large.  E.g. $ 23 $ x $ 23 $ pixels black and white for representing a handwritten digit means $ 2^{529} -1 $ parameters.
We need to have some simplifying assumptions.

\section{Structure through independence}

If $ X_{1}, \cdots , X_{n} $ are independent, then
\begin{align}
p(x_{1}, \cdots , x_{n}) &= p(x_{1})p(x_{2})\cdots p(x_{n})
\end{align}

With this assumption how many states are needed for the images? 
It is $ 2^{n} $ where $n$ is the number of pixels.
If the images are black and white, $ |Val(X_{i})| = 2 $. So $2^{n}$ entries can be described by just $n$ numbers.

In reality the independence assumption is too strong.  Because for digits handwritten, you won't get random scattering of pixels.  They will be lit up in a localised area representing the stroke of the pen.

What we do is instead to make conditional independence assumptions.

\begin{enumerate}
    \item \textbf{Chain rule} Let $ S_{1}, \cdots S_{n} $ be events, $ p(S_{i}) > 0 $.
    \newline
    $ p(S_{1} \cap S_{2} \cap \cdots \cap S_{n}) = p(S_{1})p(S_{2} | S_{1}) \cdots p(S_{n} | S_{1} \cap \cdots \cap S_{n-1})  $
    \item \textbf{Bayes' rule} Let $ S_{1}, S_{2} $ be events, $ p(S_{1}) > 0 $ and $ p(S_{2}) > 0 $.
    \newline
    $ p (S_{1} | S_{2}) = \frac{p(S_{1} \cap S_{2})}{p(S_{2})} = \frac{p(S_{2} | S_{1})p(S_{1})}{p(S_{2})}$
\end{enumerate}

In the Chain Rule, the joint distributions on the left are hard to know but the conditional probabilities (aka marginal probabilities) terms on the right are easier to obtain.  This is why the Chain Rule helps us.

Using Chain Rule
\begin{align}
    p(x_{1}, \cdots , x_{n}) &= p(x_{1})p(x_{2} | x_{1})p(x_{3} | x_{1},x_{2}) \cdots p(x_{n} | x_{1}, \cdots , x_{n-1})
\end{align}

This is the kind of factorisation used in autoregressive models.


\end{document}